# Real-Time MySQL to Snowflake CDC Pipeline Using Kafka

This project demonstrates a real-time data pipeline that captures changes from a MySQL database and streams them into Snowflake using Apache Kafka and Kafka Connect with Debezium.

> âœ… Ideal for real-time analytics, change data capture (CDC), and streaming use cases.

---

## ğŸ§± Architecture

MySQL (source DB)  
â”‚  
â”‚ CDC via Debezium (Kafka Connect Source Connector)  
â–¼  
Kafka Topics (Change Events)  
â”‚  
â”‚ Kafka Connect Snowflake Sink Connector  
â–¼  
Snowflake  

---

## âš™ï¸ Tools & Technologies

- **MySQL** â€“ Source database with transactional data  
- **Apache Kafka** â€“ Message broker for streaming data  
- **Kafka Connect** â€“ Used to move data between systems with source/sink connectors  
- **Debezium** â€“ Captures CDC events from MySQL  
- **Snowflake** â€“ Data warehouse sink  
- **Confluent Schema Registry** â€“ Manages Avro schemas for Kafka messages  
- **Kafka UI** â€“ Web UI to monitor Kafka topics and connectors  
- **Docker Compose** â€“ Used for local containerized deployment  

---

## ğŸš€ Features

- Real-time ingestion of insert, update, and delete events from MySQL  
- Schema-aware streaming using Avro and Schema Registry  
- Automatic delivery of Kafka topic data to Snowflake tables  
- End-to-end containerized deployment using Docker Compose  
- Modular configuration for connectors (JSON files)  

---

## ğŸ“ Project Structure

â”œâ”€â”€ docker-compose.yml               # Services: Kafka, Schema Registry, Kafka Connect, MySQL, Kafka UI  
â”œâ”€â”€ connectors/  
â”‚   â”œâ”€â”€ mysql-source.json            # Debezium source connector config  
â”‚   â””â”€â”€ snowflake-sink.json          # Snowflake sink connector config  
â”œâ”€â”€ init-mysql/  
â”‚   â””â”€â”€ init.sql                     # Sample data + table creation  
â”œâ”€â”€ README.md  

---

## ğŸ› ï¸ Setup Instructions

## Clone the repo

```bash
git clone https://github.com/Atef-Mousa/realtime-data-pipeline-mysql-snowflake.git
cd realtime-data-pipeline-mysql-snowflake


## Start all services

bash
Copy
Edit
docker-compose up -d

## Initialize MySQL (optional)
Exec into the MySQL container and apply the schema:

bash
Copy
Edit
docker exec -i mysql mysql -uroot -p123456 < init-mysql/init.sql

##  Register connectors

Use curl or Postman to create the connectors:

bash
Copy
Edit
curl -X POST -H "Content-Type: application/json" \
--data @connectors/mysql-source.json \
http://localhost:8083/connectors

curl -X POST -H "Content-Type: application/json" \
--data @connectors/snowflake-sink.json \
http://localhost:8083/connectors
ğŸ“Š Sample Use Case: E-Commerce Orders
When a new order is inserted into MySQL, it's streamed in real-time to Kafka.
Kafka Connect ingests this into Snowflake's raw table.
Snowflake can then transform this into analytics tables or Power BI dashboards.

ğŸ” Monitoring & Debugging
Kafka UI: http://localhost:8080

Kafka Connect logs: docker logs connect

Schema Registry: http://localhost:8081
